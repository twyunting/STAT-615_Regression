---
title: "STAT-615 Regression Exam 1"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(car)
```

# Part 2 (80 points): Exercises
1. (Use R for data analysis)
The 1974 Motor Trend US magazine contained data on fuel consumption of 32 automobiles (1973-74 models).
These data are in dataset ‘‘mtcars’’ which is already loaded in R. You can look at it with commands attach(mtcars),
names(mtcars), summary(mtcars), mtcars. Your task is to study the effect of the number of carburetors (variable
carb) on the fuel consumption in miles per gallon (variable mpg).
```{r}
mtcars
```

(a) Fit a linear regression model that can be used to predict miles per gallon based on the number of carburetors. Is the number of carburetors significant in this prediction? Report the estimated regression equation, the p-value testing significance of carburetors, and state your conclusion.
- With the p-value 0.001084, we have evidence to reject the null hypothesis in favor of an alternative hypothesis. That is, if the number of carburetors adds one unit, the miles per gallon will decrease by 2.0557 gallons.
```{r}
reg <- lm(mpg~carb, data = mtcars) # the fuel consumption in miles per gallon ~ number of carburetors
summary(reg)

```

(b) Conduct a lack-of-fit test to decide whether the relation between the fuel consumption and the number of carburetors is linear. State the test statistic, the p-value, and your conclusion. What does this test statistic
measure?
- reduced model is the usual linear regression model, SSE(Reduced) = 784.27
- full model is treating `X` as categorical and fitting the mean at each carb. SSE(Full) = 625.49 = SSE(pure error)
- The lack of fit SSE(lack of fit) = SSE(reduced) - SSE(Full) = 784.27-625.49 = 158.78
- F = (158.78/4) / (625.49)/26 = 39.695 / 24.05731 = 1.650018
- We conclude that the p-value is 0.1918, we fail to reject the H0, meaning that there is no evidence of lack of fit. Thus, using the linear regression is almost as good as using separate means at the each level of the number of carburetors. 

```{r}
reduced <- lm(mpg ~ carb, data = mtcars) # simple linear regression predicting Y in terms of X 
full <- lm(mpg ~ as.factor(carb), data = mtcars) # using group means to predict Y for each value of X, thus treating X as a categorical variable

anova(reduced, full)
```

(c) Are there any outliers in this regression analysis? Test each residual keeping the familywise error rate at a 5% level. Explain how you did the test, report the numbers that lead to your conclusion.
- At the individual level $\alpha$ = 0.05, there is a potential outlier - observation *Toyota Corolla* with the studentized residual t =  2.169892. Then, keeping the familywise error rate at the same level and using `outlierTest` for testing, 

```{r}
par(mfrow=c(2,2))
plot(reg)
```

```{r}
# Studentized residuals and testing for outliers
t <- rstudent(reg)
par(mfrow=c(1,1)) # Return to the 1x1 plot window 
# plot(t)
t[ abs(t) > 2 ]
n = length(carb)
qt( 0.025/n, n-2 )
t[ abs(t) > abs(qt( 0.025/n, n-2 )) ]
```
```{r}
outlierTest(reg)
```

2. (Use R for data analysis)
The purpose of this experiment was to assess the influence of calcium in solution on the contraction of heart muscle in rats. The left auricle of 21 rat hearts was isolated and on several occasions a constant length strip of tissue was electrically stimulated and dipped into various concentrations of calcium chloride solution, after which the shortening of the strip was accurately measured as the response.

The data are stored in R package MASS. You can look at them with commands attach(muscle), names(muscle), summary(muscle), muscle. A linear regression model is used to predict the change in length of the strip (variable Length, in mm) based on the concentration of calcium chloride solution (variable Conc, in multiples of 2.2 mM). 
```{r}
library(MASS)
muscle
```
(a) Calculate the equation of the sample regression line that predicts Length based on Conc.
- According to the summary table below, we focus on $\beta1$. If the concentration of calcium add one unit, the change in length of the strip will increase 5.4030 mm.
```{r}
reg2 <- lm(Length~Conc, data = muscle)
summary(reg2)
```

(b) Complete the ANOVA table and estimate the variance of Length.
- At the $\alpha$ - 0.05, we set H0: $\beta1$ = 0 v.s. H:a $\beta1$ != 0.
- We tested the F-value is 49.847. However, in the significant level t$\alpha$ - 0.05, the F-stat is 4.006873. 
- Because 49.847 > 4.006873 so p-value is less than 0.05, the H0 can be rejected, meaning that the linear relation between `Conc`and `Length` are found significant.
```{r}
anova(reg2)
qf(0.95, df1 = 1, df2 = 58)
```

(c) Compute a 95% confidence interval for the regression slope $\beta1$
- The 95% confidence interval for the slope (5.4030) is between 3.871132 to 6.934835
```{r}
confint(reg2, "Conc", level = 0.95)
```

(d) Test whether the slope is zero or not.
- The p-value of slope $\beta1$ was found significant in the summary table (p-value: 2.32e-09). That is, the slope is not equal to zero.

(e) Calculate the percent of total variation explained by this regression model.
- The r-square is 0.4622014, so the linear regression model has 46 % of the variance for a dependent variable `Length` that's explained by an independent variable `Conc` in the regression model.
```{r}
summary(reg2)$r.square
```

(f) Compute a 90% confidence interval for the mean Length when the concentration of calcium is 2.5.
```{r}
muscle %>%
  filter(Conc == 2.5) -> tmpDf
# Confidence intervals for regression coefficients
confint(reg2, level = 0.9)
```

(g) Compute a 90% prediction interval for Length if the concentration of calcium is 2.5.
```{r}

```

(h) Verify the standard regression assumptions - normality and homoscedasticity. Report p-values and state
your conclusions.

Here are the assumptions of simple linear regression model:
1. independent observation
2. Normally distribution
3. Equal variances
4. No influential outliers
5. Linear association between (mean) y and x. That is, residual : ri = yi - yhat i.
## Normality - using Normal Q-Q plot
- According to the normal QQ plot, there are some potential outliers in the upper extremity and lower extremity
```{r}
par(mfrow=c(2,2))
plot(reg2)
```
## Normality - Shapiro-Wilk normality test
- With large p-value 0.07566, we fail to reject the null, meaning that the data may not be non-normal.
```{r}
t <- rstudent(reg2)
shapiro.test(t)
```
## Homoscedasticity (constant variance)
- With a high p-value 0.57094,there is no evidence of non-constant variance.
```{r}
ncvTest(reg2)
```

(i) **(Graduate only)** Find the optimal Box-Cox transformation. Does it improve normality of residuals?
- A Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. In this case, we need to focus on **the largest Y-value** mapping to the X position. Thus, the optimal lambda
is somewhere between 1 to 2. Then, we zoom in the 1:2 domain with the step 0.01
```{r}
boxcox(reg2)
```
- Now, we can see that the best lambda is approximately close to 1.4 on x-axis (the peak spot). Let’s introduce a variable that is the corresponding power transform of our response Y, fit this new regression, and check residuals for
normality.
```{r}
boxcox(reg2, lambda = seq(1, 2, 0.01))
```
- Recalled: the normality test p-value of original model is **0.07566**
- According to the Shapiro-Wilk normality test table below, the p-value is **0.1533**
- Because 0.1533 > 0.07566, also the p-value is far away to the $\alpha$ level. We can conclude that the Box-Cox transformation improves residual normality.
```{r}
attach(muscle)
z <- Length^(1.4)
newReg2 <- lm(z~Conc)
shapiro.test(rstudent(newReg2))
```

(j) (Graduate only) Test the model for the lack of fit.
```{r}
reduced2 <- lm(Length ~ Conc, data = muscle)
full2 <- lm(Length ~ as.factor(Conc), data = muscle)

plot(Conc, Length)
abline(reduced2,col="red",lwd = 4)
points(Conc, predict(full2), col="blue", lwd = 10 )
```
## A rigorous F-test for the lack of fit
```{r}
anova(reduced2, full)
```

