gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text)
gutenberg_download(140) %>%
unnest_tokens(word, text)
gutenberg_download(226) %>%
unnest_tokens(word, text)
gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+"))
gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text)
gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+"))
gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
anti_join(stop_words, by = "word")
data(stop_words)
data(stop_words)
# get first data -- Upton Sinclair: "The Jungle" (1906)
gutenberg_works() %>%
filter(title == "The Jungle")
gutenberg_download(140) %>%
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
anti_join(stop_words, by = "word") ->
tidy_jungle
tidy_jungle
# get second data --  W.E.B. Du Bois: "The Quest of the Silver Fleece" (1911)
gutenberg_works() %>%
filter(str_detect(title,"The Quest of the Silver Fleece"))%>% head()
gutenberg_download(226) %>%
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
anti_join(stop_words, by = "word") ->
tidy_silver
tidy_silver
gutenberg_download(c(767, 768, 969, 1260, 9182)) %>% # brontes
unnest_tokens(word, text) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
anti_join(stop_words, by = "word") ->
tidy_bronte
tidy_bronte
tidy_silver <- gutenberg_download(226)
tidy_silver
tidy_jungle <- gutenberg_download(140)
tidy_jungle
knitr::opts_chunk$set(echo = TRUE)
library(janeaustenr)
head(orig_books)
library(janeaustenr)
austen_books() %>%  #head()
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
select(book, chapter, linenumber, everything()) ->
orig_books
head(orig_books)
austen_books() %>%  #head()
group_by(book)
tidy_silver %>% head()
tidy_silver
tidy_books <- gutenberg_download(c(140,226))
tidy_books
books_tibble <- gutenberg_download(c(140,226))
books_tibble
gutenberg_download(c(35, 36, 159, 5230)) %>%  # hgwells
unnest_tokens(word, text)
data(stop_words)
books_tibble <- books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%  # filter out any NAs
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text,  # add chapter numbers
regex("^chapter [\\divxlc]",
ignore_case = TRUE))))
books_tibble %>%
unnest_tokens(word, text)
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+"))
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word")
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word))
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%  # filter out any NAs
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text,  # add chapter numbers
regex("^chapter [\\divxlc]",
ignore_case = TRUE))))
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word))
books_tibble <- gutenberg_download(c(140,226))
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%  # filter out any NAs
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(word,  # add chapter numbers
regex("^chapter [\\divxlc]",
ignore_case = TRUE))))
books_tibble
books_tibble %>%
unnest_tokens(word, text)
books_tibble
books_tibble %>%
unnest_tokens(word, text)
books_tibble %>%
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word")
books_tibble
gutenberg_download(226)
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(word, regex("^chapter [\\divxlc]", ignore_case = TRUE))))
books_tibble
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE))))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]+", ignore_case = TRUE))))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter+", ignore_case = TRUE))))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^CHAPTER I", ignore_case = TRUE))))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc+]", ignore_case = TRUE))))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) # add chapter numbers
austen_books() %>%  #head()
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE))))
books_tibble <- gutenberg_download(c(140,226))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%  # filter out any NAs
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+"))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word")
books_tibble <- gutenberg_download(c(140,226))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word))  # filter out any NAs
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%   # filter out any NAs
filter(chapter != 0)
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%   # filter out any NAs
filter(chapter != 0)
library(tidyverse)
library(leaps)
covid_df <- read_csv("../data/covid_df_regression.csv")
covid_df %>%
select(-Province_State, -Total_Population)->covid_df
covid_df
attach(covid_df)
reg_covid_df <- lm(deaths ~.)
attach(covid_df)
reg_covid_df <- lm(deaths ~., data = covid_df)
summary(reg_covid_df)
# BIC---4
reg_model_bic <- lm(deaths ~ confirmed + Asian_alone + White_alone + IncomeTwenty, data = covid_df)
summary(reg_model_bic)
# AIC---5
reg_model_aic <- lm(deaths ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
summary(reg_model_aic)
par(mfrow = c(2,2))
plot(reg_model_aic)
library(car)
library(MASS)
t_aic <- rstudent(reg_model_aic)
shapiro.test(t_aic)
ncvTest(reg_model_aic)
boxcox(reg_model_aic)
reg_model_aic
bc_aic <- boxcox(reg_model_aic)
lambda <- bc_aic$x[which.max(bc_aic$y)]
lambda
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
z <- deaths^lambda
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
glimpse(covid_Df)
glimpse(covid_df)
z
z <- deaths^(-2)
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
z <- y^(-2)
z <- deaths^(-2)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
#knitr::include_graphics("q3.jpeg")
#knitr::include_graphics("q4.jpeg")
brand_pre <- read_table("CH06PR05.txt", col_names = FALSE)
brand_pre %>%
rename(yi = `X1`, x1 = `X2`, x2 = `X3`)->brand_pre
head(brand_pre)
bp_reg <- lm(yi~ x1, data = brand_pre)
bp_reg2 <- lm(yi ~ x1 + x2, data = brand_pre)
anova(bp_reg)
anova(bp_reg2)
#anova(bp_reg, bp_reg2)
#knitr::include_graphics("q5a.jpeg")
anova(bp_reg)
anova(bp_reg2)
bp_reg <- lm(yi~ x1, data = brand_pre)
bp_reg
bp_reg3 <- lm(yi~ x2, data = brand_pre)
anova(bp_reg3)
bp_reg3 <- lm(yi~ x2, data = brand_pre)
anova(bp_reg3)
coefficients(bp_reg3)
residuals(bp_reg3)
bp_reg3 <- lm(yi~ x1, data = brand_pre)
anova(bp_reg3)
bp_reg3 <- lm(yi~ x2, data = brand_pre)
anova(bp_reg3)
coefficients(bp_reg3)
residuals(bp_reg3)
bp_reg3 <- lm(yi~ x2, data = brand_pre)
anova(bp_reg3)
coefficients(bp_reg3)
residuals(bp_reg3)
bp_reg
anova(bp_reg)
bp_reg3 <- lm(yi~ x2, data = brand_pre)
anova(bp_reg3)
anova(bp_reg, bp_reg3)
anova(bp_reg3)
reg_model_aic
z <- deaths^(-2)
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
z <- deaths^(-2)
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
z <- deaths^lambda
reg_model_aic1 <- lm(z ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
reg_model_aic1 <- lm(deaths^(-2) ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
reg_model_aic1 <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
t <- rstudent(reg_model_aic1)
shapiro.test(reg_model_aic1)
reg_model_aic1 <- lm(deaths^(-2) ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic <- rstudent(reg_model_aic1)
shapiro.test(new_t_aic)
reg_model_aic1 <- lm(deaths^(-2) ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic <- rstudent(reg_model_aic1)
shapiro.test(new_t_aic)
reg_model_aic1 <- lm(deaths^(-2) ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic1 <- rstudent(reg_model_aic1)
shapiro.test(new_t_aic1)
# test best lambda
reg_model_aic2 <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic2 <- rstudent(reg_model_aic2)
shapiro.test(new_t_aic2)
par(mfrow = c(2,2))
plot(reg_model_aic2)
step(reg_model_aic2)
step(reg_model_aic2)
null_1 = lm(deaths^lambda ~ 1, data = covid_df)
full_1 = lm(deaths^lambda ~., data = covid_df)
step(null_1, scope = list(lower = null_1, upper = full_1), direction = "forward")
step(reg_model_aic2)
gutenberg_works() %>%
filter(title == "The Jungle")
# find first book -- Upton Sinclair: "The Jungle" (1906)
gutenberg_works() %>%
filter(title == "The Jungle")
# find second book --  W.E.B. Du Bois: "The Quest of the Silver Fleece" (1911)
gutenberg_works() %>%
filter(str_detect(title,"The Quest of the Silver Fleece"))%>% head()
books_tibble <- gutenberg_download(c(140,226))
jungle_tibble <- gutenberg_download(140)
silver_tibble <- gutenberg_download(226)
jungle_tibble
silver_tibble
jungle_tibble
silver_tibble
jungle_tibble
silver_tibble
books_tibble <- gutenberg_download(c(140,226))
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%   # filter out any NAs
filter(chapter != 0) # remove any front matter (words before Chapter 1)
books_tibble %>%
mutate(linenumber = row_number(),  # add line
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%   # filter out any NAs
filter(chapter != 0) %>%
count(chapter)# remove any front matter (words before Chapter 1)
books_tibble %>%
mutate(linenumber = row_number(),  # add line     ## regex
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%  # add chapter numbers
unnest_tokens(word, text) %>% # unnest tokens
mutate(word = str_extract(word, "[a-z']+")) %>% # remove formatting
anti_join(stop_words, by = "word") %>% # remove stop_words
filter(!is.na(word)) %>%   # filter out any NAs
filter(chapter != 0)
lambda
# test best lambda
reg_model_aic2 <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic2 <- rstudent(reg_model_aic2)
shapiro.test(new_t_aic2)
step(reg_model_aic2)
ncvTest(reg_model_aic2)
outlierTest(reg_model_aic2)
ncvTest(reg_model_aic2)
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed, data = covid_df))
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed), data = covid_df)
anova(reduced, full)
step(reg_model_aic2)
ncvTest(reg_model_aic2)
outlierTest(reg_model_aic2)
reduced = lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed), data = covid_df)
anova(reduced, full)
anova(reduced, full)
ncvTest(reg_model_aic2)
covid_df1 <- covid_df[-c(23,31,10),]
covid_df1 <- covid_df[-c(23,31,10),]
reg_model_aic1 <- lm(deaths^(-2) ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df1)
ncvTest(reg_model_aic1)
covid_df1 <- covid_df[-c(23,31,10),]
reg_model_aic_out <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df1)
ncvTest(reg_model_aic_out)
par(mfrow = c(2,2))
plot(reg_model_aic_out)
par(mfrow = c(2,2))
plot(reg_model_aic2)
newwww <- rstudent(reg_model_aic_out)
shapiro.test(newwww)
new_t_aic2 <- rstudent(reg_model_aic2)
shapiro.test(new_t_aic2)
reduced = lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed), data = covid_df)
anova(reduced, full)
reduced = lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df1)
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed), data = covid_df1)
anova(reduced, full)
step(reg_model_aic2)
ncvTest(reg_model_aic2)
outlierTest(reg_model_aic2)
reduced = lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df1)
full = lm(deaths^lambda ~ as.factor(White_alone) + as.factor(Black_or_African_American_alone) + as.factor(IncomeTwenty) + as.factor(Asian_alone) + as.factor(confirmed), data = covid_df1)
anova(reduced, full)
par(mfrow = c(2,2))
plot(reg_model_aic2)
# test best lambda
reg_model_aic2 <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic2 <- rstudent(reg_model_aic2)
shapiro.test(new_t_aic2)
par(mfrow = c(2,2))
plot(reg_model_aic2)
step(reg_model_aic2)
ncvTest(reg_model_aic2)
ncvTest(reg_model_aic2)
outlierTest(reg_model_aic2)
step(reg_model_aic2) #  AIC = 415.82 better
summary(reg_model_aic)
reg_model_aic <- lm(deaths ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
summary(reg_model_aic)
covid_df1 <- covid_df[-c(23,31,10),]
reg_model_aic_rmout <- lm(deaths^lambda ~ White_alone + Black_or_African_American_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df1)
ncvTest(reg_model_aic_rmout)  # pass constant variance, conclude homo
new_t_aic_rmout <- rstudent(reg_model_aic_rmout)
shapiro.test(new_t_aic_rmout)
reg_model_aic3 <- lm(deaths^lambda ~ White_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
reg_model_aic3 <- lm(deaths^lambda ~ White_alone + IncomeTwenty + Asian_alone + confirmed, data = covid_df)
new_t_aic3 <- rstudent(reg_model_aic3)
shapiro.test(new_t_aic3)
par(mfrow = c(2,2))
plot(reg_model_aic3)
par(mfrow = c(2,2))
plot(reg_model_aic2) ## normal qq pass normality
par(mfrow = c(2,2))
plot(reg_model_aic3)
library(tidyverse)
#library(leaps)
#library(car)
#library(MASS)
covid_df <- read_csv("../data/covid_df_regression.csv")
covid_df %>%
select(-Province_State, -Total_Population) -> covid_df
library(leaps)
library(car)
library(MASS)
library(tidyverse)
covid_df <- read_csv("../data/covid_df_regression.csv")
covid_df %>%
select(-Province_State, -Total_Population) -> covid_df
library(tidyverse)
covid_df <- read_csv("../data/covid_df_regression.csv")
covid_df %>%
select(-Province_State, -Total_Population) -> covid_df
library(leaps)
library(car)
library(MASS)
attach(covid_df)
reg_covid_df <- lm(deaths ~., data = covid_df)
summary(reg_covid_df)
reg_fit <- regsubsets(deaths ~., nvmax = 10 )
attach(covid_df)
reg_covid_df <- lm(deaths ~., data = covid_df)
summary(reg_covid_df)
reg_fit <- regsubsets(deaths ~., nvmax = 10 )
reg_fit <- regsubsets(deaths ~., nvmax = 10, data = covid_df )
summary(reg_fit)
summary(reg_fit)$adjr2 # find max adjr2---> p = 6,0.9265559
summary(reg_fit)$cp # find min cp ---> p = p = 4,3.257537
summary(reg_fit)$bic # find min bic ---> p = 4, -117.23954
which.max(summary(reg_fit)$adjr2)
which.min(summary(reg_fit)$cp)
which.min(summary(reg_fit)$bic)
install.packages(c("broom", "dbplyr", "devtools", "dplyr", "knitr", "lubridate", "reprex", "tibble", "tidyr", "tidyverse", "tinytex", "utf8"))
install.packages(c("broom", "dbplyr", "devtools", "dplyr", "knitr", "lubridate", "reprex", "tibble", "tidyr", "tidyverse", "tinytex", "utf8"))
install.packages(c("broom", "dbplyr", "devtools", "dplyr", "knitr", "lubridate", "reprex", "tibble", "tidyr", "tidyverse", "tinytex", "utf8"))
install.packages(c("broom", "dbplyr", "devtools", "dplyr", "knitr", "lubridate", "reprex", "tibble", "tidyr", "tidyverse", "tinytex", "utf8"))
library(tidyverse)
library(leaps)
install.packages("tidyverse")
library(tidyverse)
knitr::opts_chunk$set(fig.align  = "center",
fig.height = 5,
fig.width  = 6)
library(tidyverse)
setwd("D:/_Stacy/AU_Course/STAT-615_Regression_DrBarouti/project/STAT-615_final_project/analysis")
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
library(leaps)
library(car)
library(MASS)
covid_df <- read_csv("../data/covid_df_regression.csv")
covid_df %>%
select(-Province_State, -Total_Population) -> covid_df
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
data(mpg)
mpg
mpg %>%
select(model)
library(dplyr)
data(mpg)
mpg %>%
select(model)
