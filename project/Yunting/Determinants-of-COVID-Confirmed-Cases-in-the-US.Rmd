---
title: 'Determinants of COVID Confirmed Cases in the US'
subtitle: "Keywords: Web Scrapping, Linear Regression, COVID-19"
author: "Yunting Chiu, Chiyun Liu, Ana Lim"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    fig_caption: true
urlcolor: blue
linkcolor: red
---
# Install Packages
- Install the required libraries
```{r include=FALSE}
library(broom) # convert analysis objects from R into tidy tibbles
library(tidyverse) # tidy data
library(psych) # EDA
library(countrycode) # get the continents
library(lubridate) # adjust the date variable
library(usmap) # find out the US states
library(readxl) # read MS excel file
library(corrplot) #for visualization of correlation
library(leaps)  # regsubsets
library(car) # ncvTest
library(corrplot) # multicollinearity plot
library(performance) # multicollinearity table 
```

# Web Scraping COVID-19 Data
- The data source is from [Johns Hopkins University](https://github.com/CSSEGISandData). Here is the [link](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)

```{r}
url_in <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/"
```

## Data Scraping
    + time_series_covid19_confirmed_global.csv
    + time_series_covid19_deaths_global.csv
    + time_series_covid19_confirmed_US.csv
    + time_series_covid19_deaths_US.csv
```{r}
df <- tibble(file_names = c("time_series_covid19_confirmed_global.csv",
                            "time_series_covid19_deaths_global.csv",
                            "time_series_covid19_confirmed_US.csv",
                            "time_series_covid19_deaths_US.csv")) -> df
```

## Mapping Data
### Using Regex for mapping
```{r}
df %>%
  mutate(url = str_c(url_in, file_names, sep = "")) -> df
```
```{r}
df %>%
  mutate(data = map(url, ~read_csv(., na = ""))) -> df
```

```{r}
df %>%
  mutate(case_types = as.factor(str_extract(file_names, "[:alpha:]*_[gU][:alpha:]*"))) -> 
  df
# alpha = Any letter, [A-Za-z]
# reference: https://www.petefreitag.com/cheatsheets/regex/character-classes/
```
```{r}
df %>%
  dplyr::select(case_types, data) -> df
```

## Clean Data  
```{r}
df %>%
  mutate(vars = map(df$data, names)) -> df
# map(df$vars, ~unlist(.)[1:15]) for checking

fix_names <- function(df, pattern, rePattern){
  stopifnot(is.data.frame(df), is.character(pattern), is.character(rePattern))
  names(df) <- str_replace_all(names(df), pattern, rePattern)
  return(df)
}

df %>%
  mutate(data = map(data, ~fix_names(., "([ey])/", "\\1_")),
         data = map(data, ~fix_names(., "Admin2", "County")),
         data = map(data, ~fix_names(., "Long_", "Long")),
         data = map_if(data, str_detect(df$case_types, "US"),
                   ~dplyr::select(., -c("UID", "iso2", "iso3", 
                                 "code3", "FIPS", "Combined_Key"))),
         data = map_if(data, str_detect(df$case_types, "global"),
                      ~mutate(., County = "NA")),
         data = map_if(data, !str_detect(df$case_types, "deaths_US"),
                      ~mutate(., Population = 0)),
         data = map(data, ~unite(., "Country_State", 
                                 c("Country_Region", "Province_State"),
                                 remove = FALSE, na.rm = TRUE,
                                 sep = "_"))
         ) -> df

df %>%
  mutate(vars = map(df$data, names)) -> df # synchronize the vars correspondingly
# map(df$vars, ~unlist(.)) # for checking 
```

```{r}
df %>%
  mutate(data = map(data, ~pivot_longer(data = ., cols = contains("/"),
                                        names_to = "Date",
                                        values_to = "dailyValues",
                                        names_transform = list(Date = mdy)))
         ) -> df
# df$data <- map(df$data, names) # synchronize the vars correspondingly
# map(df$vars, ~unlist(.)) # for checking 

# crate a function to fix in type of Date
mdyDate <- function(df, varsDate){
  # stopifnot(is.data.frame(df), is.character(varsDate))
  df[[varsDate]] <- ymd(df[[varsDate]])
  return(df)
}

df %>%
  mutate(data = map(data, ~mdyDate(., "Date"))) -> df_long

# str(df_long) # check the data set
```

### Add Continents and fix NAs
```{r, warning = FALSE}
df_long %>%
  mutate(data = map(data, ~mutate(., Continent = countrycode(Country_Region,
                                               origin = "country.name",
                                               destination = "continent")))
         ) -> df_long
```

```{r}
df_long %>%
  mutate(data = map(data, ~mutate(., Continent = case_when(
                                               Country_Region == "Diamond Princess" ~ "Asia",
                                               Country_Region == "Kosovo" ~ "Americas",
                                               Country_Region == "MS Zaandam" ~ "Europe",
                                               TRUE ~ Continent)
                                  ))) -> df_long

map(df_long$data, ~unique(.$Continent))
```
### Unnest the Data Frames    
```{r}
# 1
df_long %>%
  unnest(cols = data) %>%
  ungroup() -> df_all

# 2
remove(df, df_long)

# 3
df_all %>%
  dplyr::select(-vars) -> df_all
```

### Get World Population Data
- source: [UN source](https://population.un.org/wpp/Download/Standard/CSV/)
```{r}
# 1
df_pop <- read_csv("../data/WPP2019_TotalPopulation.csv")
# summarize(df_pop, across(everything(), ~sum(is.na(.)))) # check NAs

# 2 
semi_join(df_pop, df_all, by = c("Location" = "Country_Region")) -> df_pop

# 3
df_pop %>% 
  mutate(rank_p = rank(-PopTotal, na.last = TRUE),
         rank_d = rank(-PopDensity, na.last = TRUE),
         PopTotal = (PopTotal*1000)) -> df_pop
```

```{r}
df_all %>%
  inner_join(df_pop, by = c("Country_Region" = "Location")) -> df_all

df_all 
```

## Tidy Data

- Because COVID-19 data is a time series data, we only focus on 2020/01/22 - 2021/01/22 for our experiment.
- According to the papers(1)(2), we refer to the author's idea to add `Races` , `Total population`, and `Incomes`variables as predictors in our COVID dataframe.
- From [JHU](https://github.com/CSSEGISandData) COVID-19 raw data, they have `latitude` and `longitude` variables, we want to keep them but they count a observation by each county. In this case, we find the `latitude` and `longitude` from [Kaggle](https://www.kaggle.com/washimahmed/usa-latlong-for-state-abbreviations) which count by each US state as an alternative one.
```{r}
# extract one year
df_all %>%
  filter(case_types == "confirmed_US" & as_date(Date) <= as_date("2021-01-22") | case_types == "deaths_US" & as_date(Date) <= as_date("2021-01-22")) -> covid


covid %>% 
  group_by(Province_State, Lat, Long) %>% ## Lat and Long not the only value per state
  filter(Province_State == "Alabama")

names(covid)

covid %>% 
  filter(Province_State == "Rhode Island")
```

### Find out each US state using usmap
```{r}
state_map <- us_map(regions = "states")
state_map %>%
  distinct(full) %>%
  rename("Province_State" = "full") -> USstates
```

### Obtain the number of confirmed cases for each state
```{r}
covid %>%
  filter(case_types == "confirmed_US" & as_date(Date) == as_date("2021-01-22")) %>%
  dplyr::select(Province_State, County, dailyValues) %>%
  group_by(Province_State) %>%
  tally(dailyValues) %>%
  right_join(USstates) %>%
  rename("confirmed" = "n") -> confirmed
```
### Obtain the number of death cases for each state
```{r}
covid %>%
  filter(case_types == "deaths_US" & as_date(Date) == as_date("2021-01-22")) %>%
   dplyr::select(Province_State, County, dailyValues) %>%
  group_by(Province_State) %>%
  tally(dailyValues) %>%
  right_join(USstates) %>%
  rename("deaths" = "n") -> deathes

full_join(confirmed, deathes) -> covid
```

### Read 2019 American community survey estimate by race by state
- Source: https://www.governing.com/now/State-Population-By-Race-Ethnicity-Data.html
```{r}
race <- read_csv("../data/2019_state_community_by_race.csv")
race %>% 
  rename(Province_State = State) -> race

covid %>% 
  left_join(race, by = "Province_State") %>%
  rename(American_Indian_and_Alaska_Native_alone = "American Indian and Alaska Native alone",
         Asian_alone = "Asian alone",
         Black_or_African_American_alone = "Black or African American alone",
         Native_Hawaiian_and_Other_Pacific_Islander_alone = "Native Hawaiian and Other Pacific Islander alone",
         Some_other_race_alone = "Some other race alone",
         Total_Population = "Total Population",
         Two_or_more_races = "Two or more races",
         White_alone = "White alone") -> covid

#race %>% 
  #anti_join(covidForRegression, by = "Province_State") #-->Check if there are some diff value of state

# The 2020 American Community Survey (ACS) 1-year estimates will be released on September 23, 2021.
# Since the 2020 survey does not yet release, we use the 2019 survey here 
```

### Read 2021 Household income

- This data is tided from[World Population Review](https://worldpopulationreview.com/state-rankings/median-household-income-by-state), and the original, and source is from [US Census](https://www.census.gov/library/visualizations/interactive/2019-median-household-income.html)

```{r}
householdIncome2021 <- read_csv("../data/MedianHouseholdIncome2021.csv")
householdIncome2021 

householdIncome2021  %>% 
  rename(Province_State = State) -> householdIncome2021

covid %>% 
  left_join(householdIncome2021, by = "Province_State") -> covid

# Recheck
#covid %>%
  #dplyr::select(Province_State, HouseholdIncome) %>%
   #dplyr::arrange(desc(HouseholdIncome))

```

# Exploratory Data Analysis 
## Data Analysis and Visualization

```{r}
str(covid)
covid %>% head()
```

### Top 5 Confirmed Covid-19 Cases (Jan.22, 2020 to Jan.22, 2021)
```{r}
covid %>%
   dplyr::select(1, 2) %>%
  arrange(desc(confirmed)) %>%
  head(5)
```

### Top 5 Death Cases (Jan.22, 2020 to Jan.22, 2021)
```{r}
covid %>%
   dplyr::select(1, 3) %>%
  arrange(desc(deaths)) %>%
  head(5)
```
### Total Population vs op 5 2021 Median Income
```{r}
covid %>%
  dplyr::arrange(desc(HouseholdIncome)) %>%
  head(5) %>%
  ggplot(aes(x = Total_Population, y = HouseholdIncome, color = Province_State)) +
  geom_point() +
  theme_bw() +
  labs(x = "Total Population", y = "2021 Median Income")
```

In order to run the linear regression model smoothly, we deleted the `Province State`variable from our covid data and saved it as `covidForRegression` dataframe. The reason of removing these two is :

- The `Province State` variable has 51 different categorical types, and each observation has its own type, making it difficult to run a regression model.
- We refer to the existing paper[1](https://doi.org/10.1002/jmv.26095)[2](https://ssrn.com/abstract=3612850) and select some similar independent variables as predictors.

```{r}
covid %>%
  dplyr::select(-Province_State) -> covidForRegression
names(covidForRegression)
```

## Descriptive Statistics
```{r}
describe(covidForRegression)
```

# Linear Model Assumptions
## Multicollinearity
### Plot
```{r}
correlation <- cor(covidForRegression) 
corrplot(correlation, method="color", addCoef.col = "black", number.cex = 0.5, type = "lower")
```

### Table
Check the inversely related values of Tolerance and VIF. **Tolerance has to be > 0.10 and VIF < 10.** If these stipulated are not fulfilled, multicollinearity is at hand.

```{r}
reg <- lm(confirmed ~ ., data = covidForRegression)
check_collinearity(reg)
```
- Remove highly correlated predictors from the model. If you have two or more factors with a high VIF, remove one from the model.
- According to the result, we firstly remove the high correlation variables: `Some_other_race_alone`, `Total_Population`, and `Two_or_more_races`, and keep the majority of race variables.
- After we removed, now the independent variables have very low correlation with each other.
```{r}
covidForRegression %>%
   # dplyr::select(-Asian_alone, -Some_other_race_alone, -Total_Population, -Two_or_more_races) 
  dplyr::select(-Total_Population, -Two_or_more_races, -Some_other_race_alone) -> covidForRegression2
```

```{r}
reg <- lm(confirmed ~ ., data = covidForRegression2)
check_collinearity(reg)
```


## Independence

We would need to know more from the data providers to really assess this. We will assume it holds.

## Linearity
The lack of fit F test works only with simple linear regression so we see the residual plots. As the residuals versus fitted plot below, there is no pattern indicates non-linearity in the data. In other words, the linearity is not violated.

```{r}
par(mfrow = c(2,2))
# summary(reg)
plot(reg)
```

### Remediation - Removing Influential Outliers
- Get other diagnostics measures
Remove the top three **cooks** and **leverage** on observations. That is, observations 5, 12, and 33 are removed.
```{r}
leverage <- hatvalues(reg)
student <- rstudent(reg)
dfs <- dffits(reg)
cooksd <- cooks.distance(reg)
data.frame(confirmed = covidForRegression2$confirmed, fitted = reg$fitted,
           residual = reg$residual, leverage, student, dffits = dfs, cooksd) -> diag

par(mfrow=c(2,2))
plot(leverage,type='h')
abline(h=0)
plot(student,type='h')
abline(h=0)
plot(dfs,type='h',ylab='dffit')
abline(h=0)
plot(cooksd,type='h')
abline(h=0)

diag %>%
  arrange(desc(leverage)) %>%
  head(3)

diag %>%
  arrange(desc(cooksd)) %>%
  head(3)
```
- Look at the Residual vs Fitted plot; linearity has occurred.

```{r}
reg1 <- lm(confirmed ~ ., data = covidForRegression2[-c(5, 12, 33),])
par(mfrow=c(2,2))
plot(reg1)
```

## Homoscedasticity
With small p-value, we have evidence that the variances are non-constant 
```{r}
# test homoscedasticity
ncvTest(reg1)
```
### Remediation - Square Root of Y
With a high p-value of 0.82952, there is no evidence of non-constant variance.
```{r}
covidForRegression2 %>%
  mutate(confirmed = sqrt(confirmed)) -> covidForRegressionSQRT

reg2 <- lm(confirmed ~ ., data = covidForRegressionSQRT[-c(5, 12, 33),])
ncvTest(reg2)
```

## Normality
- The p-value of the Shapiro-Wilk Test 0.8031 is greater than $\alpha$ 0.05 so the data is follow a normal distribution. 
```{r}
# test normality
shapiro.test(rstudent(reg2))
```

# Linear Model Selection

Our final mission is to select the **fewest** predictors and the determine by the **lowest** SSErr in the linear model.

Adding all variables as full model first. We can see that only three variables in the linear model are significant at the beginning, which is `deaths `, `Black_or_African_American_alone`, and  `White_alone`.

```{r}
summary(reg2)
```

## Exhaustive Search
Using algorithm to select the best model exhaustively. Also, to use all 11 X-variables available, change the `mv option`.Because I am too lazy to count variables, I entered a much larger number, such as my favorite number `69` to do it.
```{r}
reg_fitExhaustive <- regsubsets(confirmed ~ ., data = covidForRegressionSQRT[-c(5, 12, 33),], nvmax = 69)
summary(reg_fitExhaustive)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitExhaustive)$adjr2 
summary(reg_fitExhaustive)$cp 
summary(reg_fitExhaustive)$bic

which.max(summary(reg_fitExhaustive)$adjr2)
which.min(summary(reg_fitExhaustive)$cp) 
which.min(summary(reg_fitExhaustive)$bic)
```


## Sequential Search
We can also choose the best model by means of a stepwise procedure, starting with one model and ending with another

### Forward Method
Forward addition can be used to perform variable selection.
```{r message=FALSE}
reg_fitForward <- regsubsets(confirmed ~ ., 
                             data = covidForRegressionSQRT[-c(5, 12, 33),], 
                             method = "forward")
summary(reg_fitForward)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitForward)$adjr2 
summary(reg_fitForward)$cp 
summary(reg_fitForward)$bic

which.max(summary(reg_fitForward)$adjr2)
which.min(summary(reg_fitForward)$cp) 
which.min(summary(reg_fitForward)$bic)

```

### Backward Method
Backward elimination can be used to perform variable selection.
```{r message=FALSE}
reg_fitBackward <- regsubsets(confirmed ~ ., 
                              data = covidForRegressionSQRT[-c(5, 12, 33),], method = "backward")
summary(reg_fitBackward)
```

Select the variables and choose the optimal model
```{r}
summary(reg_fitBackward)$adjr2 
summary(reg_fitBackward)$cp 
summary(reg_fitBackward)$bic

which.max(summary(reg_fitBackward)$adjr2)
which.min(summary(reg_fitBackward)$cp) 
which.min(summary(reg_fitBackward)$bic)

```

## Stepwise Method

We use algorithm to considers either adding or removing variables at each step to final the best model. Lower AIC (Akaike information criterion) values indicate a better-fit model.
```{r}
null = lm(confirmed ~ 1, data = covidForRegressionSQRT[-c(5, 12, 33),])
full = lm(confirmed ~ ., data = covidForRegressionSQRT[-c(5, 12, 33),])
step(null, scope = list(lower = null, upper = full), direction = "both")
```


## Comparison
We will select the fewest variable for each set, compare their MSE, and finally select the one with the local minimum MSE.

1. From exhaustive search, the fewest predictors is 3 in smallest Mallows Cp.
```{r}
regExhaustive <- lm(confirmed ~ deaths + White_alone + HouseholdIncome
                    , data = covidForRegressionSQRT[-c(5, 12, 33),])
summary(regExhaustive)
```

2. For forward method in sequential search, the fewest predictors is 3 in smallest Mallows Cp.
```{r}
regForward <- lm(confirmed ~ deaths + White_alone + HouseholdIncome
                    , data = covidForRegressionSQRT[-c(5, 12, 33),])
summary(regForward)
```

3. For backward method in sequential search, the fewest predictors is 3 in smallest Mallows Cp.
```{r}
regBackward <-  lm(confirmed ~ deaths + Black_or_African_American_alone + White_alone
                    , data = covidForRegressionSQRT[-c(5, 12, 33),])
summary(regBackward)
```

4. For stepwise method, the lowest AIC value is **443.88**, reporting the best model with 5 variables is: `lm(formula = confirmed ~ White_alone + deaths + HouseholdIncome + Black_or_African_American_alone + American_Indian_and_Alaska_Native_alone, data = covidForRegressionSQRT[-c(5, 12, 33), ])`.
```{r}
regStepwise <- lm(formula = confirmed ~ White_alone + deaths + HouseholdIncome + 
                  Black_or_African_American_alone + American_Indian_and_Alaska_Native_alone, 
                  data = covidForRegressionSQRT[-c(5, 12, 33), ])
summary(regStepwise)

```

5. Let us start to find the minimum MSE
```{r}
#calculate MSE
anova(regExhaustive)[3, ] # 49620
anova(regForward)[3, ] # 49620
anova(regBackward)[3, ] # 121185
anova(regStepwise)[3, ] # 49620
```

## Final Selection
Because `regBackward` has the highest MSE, it should be removed first. `RegStepwise` has more predictors than the other three, so remove it. The remaining two model `regExhaustive` and `regForward` not only have a same predictors but also have the minimum MSE so our final predictors of square root of COVID confirmed cases by States is `deaths`, `White_alone`, and `HouseholdIncome`.
```{r}
finalReg <- lm(confirmed ~ deaths + White_alone + HouseholdIncome
                    , data = covidForRegressionSQRT[-c(5, 12, 33),])
summary(finalReg) 
```

# Model Equation
$$
\hat{Square Root of Confirmed} =  505.9 + 0.01647deaths + 0.0000368White_alone - 0.003031HouseholdIncome
$$

# References
(1) Sehra, S., Fundin, S., Lavery, C., & Baker, J. (2020). Differences in race and other state‐level characteristics and associations with mortality from COVID‐19 infection. *Journal of Medical Virology, 92*(11), 2406–2408. https://doi.org/10.1002/jmv.26095

(2) Sa, Filipa G., Socioeconomic Determinants of Covid-19 Infections and Mortality: Evidence from England and Wales (May 2020). CEPR Discussion Paper No. DP14781, Available at SSRN: https://ssrn.com/abstract=3612850

