---
title: "Homework 6"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
linkcolor: red
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
1. (**6.3**) A student stated: “Adding predictor variables to a regression model can never reduce
$R^2$, so we should include all available predictor variables in the model.” Comment.
- Not really. If the model have a bigger $R^2$ value, which means the fitter is better. However, better fitting does not necessarily imply the fitted model is a useful one.

2. (**6.4**) Why is it not meaningful to attach a sign to the coefficient of multiple correlation R,
although we do so for the coefficient of simple correlation $r_{12}$?
- The range of simple correlation coefficient is -1 to 1. The multiple correlation shows how any variable can be predicted by using a set of other variables. There will be several independent variables, each of which will have a different effect. The multiple correlation coefficient will be more complicated than the simple one because the range is not limited to -1 to 1.

3. (**6.27**) In a small-scale regression study, the following data were obtained,

|Y    |     X1   |    X2|
|:--------|:------:|--------:|
|42.0 |7.0| 33.0|
|33.0 |4.0 |41.0|
|75.0| 16.0| 7.0|
|28.0| 3.0| 49.0|
|91.0 |21.0 |5.0|
|55.0| 8.0 |31.0|

Assume the standard multiple regression model with independent normal error terms. Compute **b, e, H,** SSErr, $R^2$, $s^2_b$, $\hat{Y}$ for $X1$ = 10, $X2$ = 30. You can do the computations using software or by hand, although it would be lengthy to do them by hand.

- The first column is `scalar` (1, 1, 1, 1, 1, 1)
```{r}
scalar <- c(1, 1, 1, 1, 1, 1)
Y <-  c(42, 33, 75, 28, 91, 55)
X1 <-  c(7, 4, 16, 3, 21, 8)
X2 <-  c(33, 41, 7, 49, 5, 31)

# namely X is the matrix
X <- matrix(c(scalar, X1, X2), 6, 3, byrow = FALSE)
X
```

## b: slope
$$
\boldsymbol{\hat{\beta}} = \boldsymbol{(X^TX)}^{-1}\boldsymbol{(X^TY)}
$$

```{r}
library(matlib)
b = inv(t(X) %*% X) %*% t(X) %*% Y
b
```
## H: Hat 
- H is called the hat-matrix (because it transforms $y$ to $\hat{y}$)
$$
H = X\boldsymbol{(X^TX)}^{-1}\boldsymbol{X^T}
$$
```{r}
H = X %*% inv(t(X) %*% X) %*% t(X)
H
```


## e: resuduals
$$
e = Y -\hat{Y} = Y - H * Y
$$
```{r}
e <- Y - H %*% Y
e
```
## SSErr
$$
SSE = e^T * e
$$

- **SSErr** is error sum of squares. 
```{r}
SSE <- t(e) %*% e
SSE
# SSErr <- sum(e^2)
# SSErr
```

## **$R^2$**
$$
R^2 = \frac{SSRegression}{SSTotal} = 1 - \frac{SSError}{SSTotal}
$$
```{r}
SST <- sum((Y - mean(Y))^2) # total sum of squares
SST
R2 <- 1 - SSE/SST # R^2
R2
```

## **$s^2_b$**
$$
MSE = \frac{SSE}{n-2}
$$
```{r}
MSE <- SSE/ (6-2)
MSE
```
$$
S^2\{b\} = MSE(X^T * X)^{-1}
$$
```{r}
s2b <- MSE[1,1] * inv(t(X) %*% X)
s2b
```

## E{Y|X1 = 10, X2 = 30}
- The equation below is an unbiased estimator of estimated of Y_n.
$$
\hat{Y}_h = X_h^T * b
$$
```{r}
Xone <- c(1, 10, 30)
yhat <- t(Xone) %*% b
yhat
```


- Reference: 
  + https://stats.stackexchange.com/questions/352130/converting-the-beta-coefficient-from-matrix-to-scalar-notation-in-ols-regression
  + https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf
  + http://fs2.american.edu/baron/www/Handouts/review%20-%20regression.pdf

4. (Computer project, **#6.5—#6.8**) Dataset “Brand preference” is available on our Blackboard, on http://statweb.lsu.edu/EXSTWeb/StatLab/DataSets/NKNWData/CH06PR05.txt, and here:
```{r}
brand <- read.table("./data/CH06PR05.txt")
brand %>%
  rename(brand = "V1", moisture = "V2", sweetness = "V3") -> brand
brand
# Y, X1, X2
```
It was collected to study the relation between degree of brand liking (Y ) and moisture content
(X1) and sweetness (X2) of the product.
(a) Fit a regression model to these data and state the estimated regression function. Interpret b1.
$$
\hat{Y} = 37.6500 + 4.4250X1 + 4.3750X2
$$
- The slope $b_1$ is 4.425, meaning that the mean response `degree of brand liking` is increase by 4.425 with 1 unit increase of `moisture` when `sweetness` is held constant.
```{r}
reg <- lm(brand ~ moisture + sweetness, data = brand)
summary(reg)
```

(b) Obtain residual plots. What information do they provide? Plot residuals against fitted values, against each predictor, and against the product of predictors.
- We need to make residual plots to see if the data points meet the linear assumptions or not.
- Residuals vs Fitted plot: a strong pattern indicates non-linearity in the data
- Normal QQ plot: Some outliers can be found in the right upper area. It is necessary to conduct further testing.
- Scale-Location: The residuals seem symmetric and the red line is approximately horizontal. However,  the average magnitude of the standardized residuals is not changing much as a function of the fitted values.
- Residual vs Leverage: There are some potential outliers can be found in the right side.
Reference: https://boostedml.com/2019/03/linear-regression-plots-scale-location-plot.html

```{r}
par(mfrow = c(2, 2))
plot(reg)
```

(c) Test homoscedasticity.
- With a high p-value 0.42877, there is no evidence of non-constant variance.
```{r}
library(car)
ncvTest(reg)
```

(d) Conduct a formal lack of fit test. 
- We conclude that the p-value is 0.3843, we fail to reject the H0, meaning that there is no evidence of lack of fit. Thus, there should be no significant difference in error between estimates from the reduced (`reg`) model and the full model.
- Reference: https://stats.stackexchange.com/questions/339331/difference-between-full-model-and-reduced-model-in-one-way-anova
$$
Full Model: Y_{ij} = \mu_j + e_{ij}
$$
$$
Reduced Model: Y_{ij} = \mu + e_{ij}
$$
```{r}
full <- lm(brand ~ as.factor(moisture) + as.factor(sweetness), data = brand)
anova(reg, full)
```

(e) Test whether the proposed linear regression model is significant. What do the results of the ANOVA F-test imply about the slopes?
- Let we set H0: $\beta1 = \beta2 = 0$ vs Ha: $\beta1 = \beta2 \neq0$.
- We found both p-values ($\beta1$ and $\beta2$) are smaller than significant level, implying that we are in favor of Ha.
```{r}
anova(reg)
summary(reg)
```

(f) Estimate both slopes simultaneously using the Bonferroni procedure with at least a 99 percent confidence level.
- The Bonferroni test is a statistical test used to reduce the instance of a false positive (typr 1 error)
- To preform the Bonferroni correction, we need to divide the $\alpha$ level by the number of tests that being executed.
- We need to test 2 slopes, and the significance level is 0.01. So, 0.01/2 = 0.005. Throughout the Bonferroni procedure, the adjusted $alpha$ level is 0.005 with two slopes.
- Reference: https://toptipbio.com/bonferroni-correction-method/
```{r}
confint(reg, level = (1-0.005))
```

(g) Report R2 and adjusted R2. How are they interpreted here?
- It means that 95 % of the variation in the `brand` is explained by the `moisture` and `sweetness` independent variables.
```{r}
summary(reg)$r.square
```
- It means that 94 % of the variation in the `brand` is explained by model addition that are significant of `moisture` and `sweetness`.
```{r}
summary(reg)$adj.r.square
```
- Reference: https://discuss.analyticsvidhya.com/t/difference-between-r-square-and-adjusted-r-square/264/2

(h) Calculate the squared correlation coefficient between $Yi$ and $\hat{Y}$. Compare with part (g).
- The squared correlation coefficient between $Yi$ and $\hat{Y}$ is $R^2$, which is 0.952059.

(i) Obtain a 99% confidence interval for E(Y) when X1 = 5 and X2 = 4. Interpret it.
- We have 99 % confidence to conclude that the **mean** of `degree of brand liking` is between 73.88111 to 80.66889 when the `moisture content` is 5 and the `sweetness of the product` is 4.
```{r}
predict(reg, tibble(moisture = 5, sweetness = 4), interval = "confidence", level = 0.99)
```

(j) Obtain a 99% prediction interval for a new observation Y when X1 = 5 and X2 = 4. Interpret it.
- We have 99 % confidence to conclude that **a next new observation ** of `degree of brand liking` is between 73.88111 to 80.66889 when the `moisture content` is 5 and the `sweetness of the product` is 4.
```{r}
predict(reg, tibble(moisture = 5, sweetness = 4), interval = "prediction", level = 0.99)
```

