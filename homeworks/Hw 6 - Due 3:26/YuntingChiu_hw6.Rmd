---
title: "Homework 6"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
linkcolor: red
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
1. (**6.3**) A student stated: “Adding predictor variables to a regression model can never reduce
$R^2$, so we should include all available predictor variables in the model.” Comment.
- Not really. If the model have a bigger $R^2$ value, which means the fitter is better. However, better fitting does not necessarily imply the fitted model is a useful one.

2. (**6.4**) Why is it not meaningful to attach a sign to the coefficient of multiple correlation R,
although we do so for the coefficient of simple correlation $r_{12}$?
- The range of simple correlation coefficient is -1 to 1. The multiple correlation shows how any variable can be predicted by using a set of other variables. There will be several independent variables, each of which will have a different effect. The multiple correlation coefficient will be more complicated than the simple one because the range is not limited to -1 to 1.

3. (**6.27**) 
4. (Computer project, **#6.5—#6.8**) Dataset “Brand preference” is available on our Blackboard, on http://statweb.lsu.edu/EXSTWeb/StatLab/DataSets/NKNWData/CH06PR05.txt,
and here:
```{r}
brand <- read.table("./data/CH06PR05.txt")
brand %>%
  rename(brand = "V1", moisture = "V2", sweetness = "V3") -> brand
brand
# Y, X1, X2
```
It was collected to study the relation between degree of brand liking (Y ) and moisture content
(X1) and sweetness (X2) of the product.
(a) Fit a regression model to these data and state the estimated regression function. Interpret b1.
$$
\hat{Y} = 37.6500 + 4.4250X1 + 4.3750X2
$$
- The slope $b_1$ is 4.425, meaning that the mean response `degree of brand liking` is increase by 4.425 with 1 unit increase of `moisture` when `sweetness` is held constant.
```{r}
reg <- lm(brand ~ moisture + sweetness, data = brand)
summary(reg)
```

(b) Obtain residual plots. What information do they provide? Plot residuals against fitted values, against each predictor, and against the product of predictors.
- We need to make residual plots to see if the data points meet the linear assumptions or not.
- Residuals vs Fitted plot: a strong pattern indicates non-linearity in the data
- Normal QQ plot: Some outliers can be found in the right upper area. It is necessary to conduct further testing.
- Scale-Location: The residuals seem symmetric and the red line is approximately horizontal. However,  the average magnitude of the standardized residuals is not changing much as a function of the fitted values.
- Residual vs Leverage: There are some potential outliers can be found in the right side.
Reference: https://boostedml.com/2019/03/linear-regression-plots-scale-location-plot.html

```{r}
par(mfrow = c(2, 2))
plot(reg)
```

(c) Test homoscedasticity.
- With a high p-value 0.42877, there is no evidence of non-constant variance.
```{r}
library(car)
ncvTest(reg)
```

(d) Conduct a formal lack of fit test. 
- We conclude that the p-value is 0.3843, we fail to reject the H0, meaning that there is no evidence of lack of fit. Thus, there should be no significant difference in error between estimates from the reduce model and the full model.
- Reference: https://stats.stackexchange.com/questions/339331/difference-between-full-model-and-reduced-model-in-one-way-anova
$$
Full Model: Y_{ij} = \mu_j + e_{ij}
$$
$$
Reduced Model: Y_{ij} = \mu + e_{ij}
$$
```{r}
full <- lm(brand ~ as.factor(moisture) + as.factor(sweetness), data = brand)
anova(reg, full)
```

(e) Test whether the proposed linear regression model is significant. What do the results of the ANOVA F-test imply about the slopes?
- 
```{r}
summary(reg)
```

(f) Estimate both slopes simultaneously using the Bonferroni procedure with at least a 99 percent confidence level.
(g) Report R2 and adjusted R2. How are they interpreted here?
(h) Calculate the squared correlation coefficient between $Yi$ and $\hat{Y}$. Compare with part (g).